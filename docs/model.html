---

title: Title
keywords: fastai
sidebar: home_sidebar

summary: "summary"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/04_model.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="5455bfa8-9abf-495b-ad02-c8a6b0ccb855"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#5455bfa8-9abf-495b-ad02-c8a6b0ccb855');

            setTimeout(function() {
                var nbb_cell_id = 5;
                var nbb_unformatted_code = "# export\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nfrom fastai2.text.models import RNNDropout";
                var nbb_formatted_code = "# export\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nfrom fastai2.text.models import RNNDropout";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Model">Model<a class="anchor-link" href="#Model">&#182;</a></h1><p>We're attempting to build a model that can effectively learn two parallel signals (pitch and duration) at the same time, with a single loss function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>But before, we'll take a piece from Fastai v1 that I couldn't find in Fastai2, the Linear Decoder:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="3caf4fe4-ea20-4705-b92e-d1f4321c38b7"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#3caf4fe4-ea20-4705-b92e-d1f4321c38b7');

            setTimeout(function() {
                var nbb_cell_id = 6;
                var nbb_unformatted_code = "# export\nclass LinearDecoder(nn.Module):\n    initrange = 0.1\n\n    def __init__(\n        self,\n        n_out: int,\n        n_hid: int,\n        output_p: float,\n        tie_encoder=None,\n        bias: bool = True,\n    ):\n        super().__init__()\n        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n        self.output_dp = RNNDropout(output_p)\n        if bias:\n            self.decoder.bias.data.zero_()\n        if tie_encoder:\n            self.decoder.weight = tie_encoder.weight\n\n    def forward(self, input: Tensor) -> Tensor:\n        output = self.output_dp(input)\n        decoded = self.decoder(\n            output.view(output.size(0) * output.size(1), output.size(2))\n        )\n        return decoded";
                var nbb_formatted_code = "# export\nclass LinearDecoder(nn.Module):\n    initrange = 0.1\n\n    def __init__(\n        self,\n        n_out: int,\n        n_hid: int,\n        output_p: float,\n        tie_encoder=None,\n        bias: bool = True,\n    ):\n        super().__init__()\n        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n        self.output_dp = RNNDropout(output_p)\n        if bias:\n            self.decoder.bias.data.zero_()\n        if tie_encoder:\n            self.decoder.weight = tie_encoder.weight\n\n    def forward(self, input: Tensor) -> Tensor:\n        output = self.output_dp(input)\n        decoded = self.decoder(\n            output.view(output.size(0) * output.size(1), output.size(2))\n        )\n        return decoded";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LinearDecoder" class="doc_header"><code>class</code> <code>LinearDecoder</code><a href="https://github.com/codegram/neuralmusic/tree/master/neuralmusic/model.py#L12" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LinearDecoder</code>(<strong><code>n_out</code></strong>:<code>int</code>, <strong><code>n_hid</code></strong>:<code>int</code>, <strong><code>output_p</code></strong>:<code>float</code>, <strong><code>tie_encoder</code></strong>=<em><code>None</code></em>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>True</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And finally, the model:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="b426918b-d37e-4b36-9d6b-3ed50722ecd1"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#b426918b-d37e-4b36-9d6b-3ed50722ecd1');

            setTimeout(function() {
                var nbb_cell_id = 7;
                var nbb_unformatted_code = "# export\nclass TheModel(nn.Module):\n    def __init__(self, pitch_len, duration_len, emb_size=1000, rnn_size=1200, rnn_layers=3, dropout=0.0):\n        super().__init__()\n\n        self.pitch_emb = nn.Embedding(num_embeddings=pitch_len, embedding_dim=emb_size, padding_idx=1)\n        self.duration_emb = nn.Embedding(num_embeddings=duration_len, embedding_dim=emb_size, padding_idx=1)\n\n        self.hidden_size = rnn_size\n        self.layers = rnn_layers\n        self.pitch_rnn = nn.GRU(input_size=emb_size, hidden_size=rnn_size, num_layers=rnn_layers, batch_first=False, bidirectional=False)\n        self.duration_rnn = nn.GRU(input_size=emb_size, hidden_size=rnn_size, num_layers=rnn_layers, batch_first=False, bidirectional=False)\n\n        self.pitch_dec = LinearDecoder(n_out=pitch_len, n_hid=rnn_size + rnn_size, output_p=dropout)\n        self.duration_dec = LinearDecoder(n_out=duration_len, n_hid=rnn_size + rnn_size, output_p=dropout)\n\n        self.reset()\n\n    def forward(self, x):\n        pitches, durations = x.transpose(0,1)\n\n        pitch_emb = self.pitch_emb(pitches).transpose(0,1)\n        duration_emb = self.duration_emb(durations).transpose(0,1)\n\n        if self.pitch_hid is None:\n          self.pitch_hid = self.init_hidden(self.layers, pitches.size(0), self.hidden_size)\n        if self.duration_hid is None:\n          self.duration_hid = self.init_hidden(self.layers, durations.size(0), self.hidden_size)\n\n        pitch, self.pitch_hid = self.pitch_rnn(pitch_emb, self.pitch_hid)\n        duration, self.duration_hid = self.duration_rnn(duration_emb, self.duration_hid)\n\n        together = torch.cat([pitch, duration], dim=2)\n\n        pitch_decoded = self.pitch_dec(together)\n        duration_decoded = self.duration_dec(together)\n\n        self.pitch_hid.detach_()\n        self.duration_hid.detach_()\n\n        pitch_out = pitch_decoded.view(pitches.size(0), pitches.size(1), -1)\n        duration_out = duration_decoded.view(durations.size(0), durations.size(1), -1)\n\n        return pitch_out, duration_out\n\n    def reset(self):\n        self.pitch_hid = None\n        self.duration_hid = None\n\n    def init_hidden(self, layers, batch_size, hidden_size):\n        weight = next(self.parameters()).data\n        return weight.new(layers, batch_size, hidden_size).zero_()";
                var nbb_formatted_code = "# export\nclass TheModel(nn.Module):\n    def __init__(\n        self,\n        pitch_len,\n        duration_len,\n        emb_size=1000,\n        rnn_size=1200,\n        rnn_layers=3,\n        dropout=0.0,\n    ):\n        super().__init__()\n\n        self.pitch_emb = nn.Embedding(\n            num_embeddings=pitch_len, embedding_dim=emb_size, padding_idx=1\n        )\n        self.duration_emb = nn.Embedding(\n            num_embeddings=duration_len, embedding_dim=emb_size, padding_idx=1\n        )\n\n        self.hidden_size = rnn_size\n        self.layers = rnn_layers\n        self.pitch_rnn = nn.GRU(\n            input_size=emb_size,\n            hidden_size=rnn_size,\n            num_layers=rnn_layers,\n            batch_first=False,\n            bidirectional=False,\n        )\n        self.duration_rnn = nn.GRU(\n            input_size=emb_size,\n            hidden_size=rnn_size,\n            num_layers=rnn_layers,\n            batch_first=False,\n            bidirectional=False,\n        )\n\n        self.pitch_dec = LinearDecoder(\n            n_out=pitch_len, n_hid=rnn_size + rnn_size, output_p=dropout\n        )\n        self.duration_dec = LinearDecoder(\n            n_out=duration_len, n_hid=rnn_size + rnn_size, output_p=dropout\n        )\n\n        self.reset()\n\n    def forward(self, x):\n        pitches, durations = x.transpose(0, 1)\n\n        pitch_emb = self.pitch_emb(pitches).transpose(0, 1)\n        duration_emb = self.duration_emb(durations).transpose(0, 1)\n\n        if self.pitch_hid is None:\n            self.pitch_hid = self.init_hidden(\n                self.layers, pitches.size(0), self.hidden_size\n            )\n        if self.duration_hid is None:\n            self.duration_hid = self.init_hidden(\n                self.layers, durations.size(0), self.hidden_size\n            )\n\n        pitch, self.pitch_hid = self.pitch_rnn(pitch_emb, self.pitch_hid)\n        duration, self.duration_hid = self.duration_rnn(duration_emb, self.duration_hid)\n\n        together = torch.cat([pitch, duration], dim=2)\n\n        pitch_decoded = self.pitch_dec(together)\n        duration_decoded = self.duration_dec(together)\n\n        self.pitch_hid.detach_()\n        self.duration_hid.detach_()\n\n        pitch_out = pitch_decoded.view(pitches.size(0), pitches.size(1), -1)\n        duration_out = duration_decoded.view(durations.size(0), durations.size(1), -1)\n\n        return pitch_out, duration_out\n\n    def reset(self):\n        self.pitch_hid = None\n        self.duration_hid = None\n\n    def init_hidden(self, layers, batch_size, hidden_size):\n        weight = next(self.parameters()).data\n        return weight.new(layers, batch_size, hidden_size).zero_()";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TheModel" class="doc_header"><code>class</code> <code>TheModel</code><a href="https://github.com/codegram/neuralmusic/tree/master/neuralmusic/model.py#L40" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TheModel</code>(<strong><code>pitch_len</code></strong>, <strong><code>duration_len</code></strong>, <strong><code>emb_size</code></strong>=<em><code>1000</code></em>, <strong><code>rnn_size</code></strong>=<em><code>1200</code></em>, <strong><code>rnn_layers</code></strong>=<em><code>3</code></em>, <strong><code>dropout</code></strong>=<em><code>0.0</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>

</div>

</div>

</div>
</div>

</div>
</div>
 

