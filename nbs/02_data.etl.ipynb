{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# hide\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# hide\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Collection\n",
    "\n",
    "import prefect\n",
    "import fastparquet\n",
    "from spell.metrics import send_metric\n",
    "from prefect import task, Flow\n",
    "from prefect.engine.signals import SKIP\n",
    "from prefect.tasks.shell import ShellTask\n",
    "\n",
    "from neuralmusic.midi import parse_midi_file\n",
    "\n",
    "log = logging.getLogger(\"data.etl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# default_exp data.etl\";\n",
       "                var nbb_formatted_code = \"# default_exp data.etl\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default_exp data.etl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL to process MIDI files\n",
    "\n",
    "> Turning a bunch of MIDI files into parquet data\n",
    "\n",
    "This ETL takes a tar.gz'd file full of arbitrary midi files and outputs a bunch of parquet files containing neat dataframes with pitch, durations and velocity triplets for each song.\n",
    "\n",
    "First we need some global variables and reporting tools to log the progress. This ETL may run locally or on Spell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "total_songs = 0\n",
    "malformed_songs = 0\n",
    "valid_songs = 0\n",
    "valid_rows = 0\n",
    "\n",
    "started_at = None\n",
    "\n",
    "\n",
    "def init_stats():\n",
    "    \"\"\"\n",
    "    Resets reporting stats to zero.\n",
    "    \"\"\"\n",
    "    global total_songs\n",
    "    global valid_songs\n",
    "    global valid_rows\n",
    "    global malformed_songs\n",
    "    global started_at\n",
    "\n",
    "    total_songs = 0\n",
    "    malformed_songs = 0\n",
    "    valid_songs = 0\n",
    "    valid_rows = 0\n",
    "\n",
    "    started_at = time.time()\n",
    "\n",
    "\n",
    "def report(logger):\n",
    "    \"\"\"\n",
    "    Reports current metrics, either to Spell or to a logger.\n",
    "    \"\"\"\n",
    "    elapsed = time.time() - started_at\n",
    "    send_metric(\"Total Songs\", total_songs)\n",
    "    send_metric(\"Malformed Songs\", malformed_songs)\n",
    "    send_metric(\"Songs\", valid_songs)\n",
    "    send_metric(\"Total Songs / second\", (total_songs / elapsed))\n",
    "    send_metric(\"Rows / second\", (valid_rows / elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untar'ing the file\n",
    "\n",
    "The first step is to untar the file containing the MIDI files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@task\n",
    "def untar_cmd(file_path: str, outdir: str) -> str:\n",
    "    \"\"\"\n",
    "    Untars a .tar.gz file onto a directory (will create it if it does not exist).\n",
    "    \"\"\"\n",
    "    if os.path.exists(outdir):\n",
    "        raise SKIP(\"Output directory already exists.\")\n",
    "    return f\"mkdir -p {outdir} && tar -zxf {file_path} -C {outdir}\"\n",
    "\n",
    "\n",
    "untar = ShellTask(name=\"untar_task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning the files in minibatches\n",
    "\n",
    "Since the tar.gz file may contain a huge amount of MIDI files, we'll partition those files into minibatches that we can process in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@task(skip_on_upstream_skip=False)\n",
    "def partition_files(\n",
    "    data_path: str, partition_size: int = 100, min_partitions: int = 4\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Partitions the midi files in data_path into chunks.\n",
    "    \"\"\"\n",
    "    midi_files = list(Path(data_path).glob(\"**/*.mid\"))\n",
    "    n = len(midi_files)\n",
    "    if (n / partition_size) < min_partitions:\n",
    "        partition_size = math.ceil(n / min_partitions)\n",
    "    logger = prefect.context.get(\"logger\")\n",
    "    logger.info(\n",
    "        f\"Processing {n} MIDI files partitioned into groups of {partition_size}\"\n",
    "    )\n",
    "    return [\n",
    "        midi_files[i : i + partition_size]\n",
    "        for i in range(0, len(midi_files), partition_size)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing a minibatch\n",
    "\n",
    "For each minibatch, we'll go through its MIDI files, parse them, and write them to a separate Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@task\n",
    "def process_and_write(mini_batch: Collection[str], outdir: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Parses a mini batch of MIDI files and writes the results to a parquet file.\n",
    "    The filename is determined by `map_index`. Returns the amount of notes it parsed.\n",
    "    \"\"\"\n",
    "    frame_no = prefect.context.get(\"map_index\")\n",
    "    Path(outdir).mkdir(parents=True, exist_ok=True)\n",
    "    outfile = f\"{outdir}/out_{frame_no}.parq\"\n",
    "\n",
    "    logger = prefect.context.get(\"logger\")\n",
    "\n",
    "    should_append = False\n",
    "\n",
    "    global total_songs\n",
    "    global valid_songs\n",
    "    global valid_rows\n",
    "    global malformed_songs\n",
    "\n",
    "    for file in mini_batch:\n",
    "        df = parse_midi_file(file)\n",
    "        if df is not None:\n",
    "            valid_songs += 1\n",
    "            valid_rows += len(df)\n",
    "\n",
    "            fastparquet.write(outfile, df, compression=\"SNAPPY\", append=should_append)\n",
    "            del df\n",
    "            should_append = True\n",
    "        else:\n",
    "            malformed_songs += 1\n",
    "            logger.warning(f\"[Minibatch {frame_no}] {file} could not be processed.\")\n",
    "\n",
    "        total_songs += 1\n",
    "        report(logger)\n",
    "\n",
    "    return outfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the parquet files\n",
    "\n",
    "Once we have all the minibatches in separate parquet files, merging them into a single dataset is trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@task\n",
    "def combine_parquet_files(files: Collection[str]) -> None:\n",
    "    \"\"\"\n",
    "    Combines N parquet files with the same schema into another one.\n",
    "    \"\"\"\n",
    "    fastparquet.writer.merge(files, verify_schema=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "Now we can build the ETL flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def build_etl(cfg):\n",
    "    \"\"\"\n",
    "    Builds the ETL flow.\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        cfg.tar_gz_path or cfg.midi_path\n",
    "    ), \"Config not found: data.etl.tar_gz_path or data.etl.midi_path\"\n",
    "    assert cfg.outdir, \"Config not found: data.etl.outdir\"\n",
    "\n",
    "    with Flow(\"Neuralmusic Data ETL\") as flow:\n",
    "        if cfg.tar_gz_path:\n",
    "            tar_gz_path = Path(cfg.tar_gz_path).resolve()\n",
    "            assert tar_gz_path.exists(), f\"{tar_gz_path} does not exist\"\n",
    "            command = untar_cmd(str(tar_gz_path), \"data\")\n",
    "            untarred = untar(command=command)\n",
    "            midi_path = \"data\"\n",
    "        else:\n",
    "            assert (\n",
    "                Path(cfg.midi_path).resolve().exists()\n",
    "            ), f\"{cfg.midi_path} does not exist\"\n",
    "            midi_path = cfg.midi_path\n",
    "\n",
    "        mini_batches = partition_files(\n",
    "            midi_path,\n",
    "            partition_size=cfg.partition_size,\n",
    "            upstream_tasks=([untarred] if cfg.tar_gz_path else []),\n",
    "        )\n",
    "\n",
    "        partitions = process_and_write.map(mini_batches, outdir=cfg.outdir)\n",
    "\n",
    "        combine_parquet_files(partitions)\n",
    "\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the ETL\n",
    "\n",
    "The ETL accepts a `tar.gz` file input containing MIDI files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-21 16:07:51,597] INFO - prefect.FlowRunner | Beginning Flow run for 'Neuralmusic Data ETL'\n",
      "[2019-12-21 16:07:51,600] INFO - prefect.FlowRunner | Starting flow run.\n",
      "[2019-12-21 16:07:51,626] INFO - prefect.TaskRunner | Task 'untar_cmd': Starting task run...\n",
      "[2019-12-21 16:07:51,643] INFO - prefect.TaskRunner | Task 'untar_cmd': finished task run for task with final state: 'Skipped'\n",
      "[2019-12-21 16:07:51,665] INFO - prefect.TaskRunner | Task 'untar_task': Starting task run...\n",
      "[2019-12-21 16:07:51,677] INFO - prefect.TaskRunner | Task 'untar_task': finished task run for task with final state: 'Skipped'\n",
      "[2019-12-21 16:07:51,699] INFO - prefect.TaskRunner | Task 'partition_files': Starting task run...\n",
      "[2019-12-21 16:07:51,702] INFO - prefect.Task: partition_files | Processing 4 MIDI files partitioned into groups of 1\n",
      "[2019-12-21 16:07:51,719] INFO - prefect.TaskRunner | Task 'partition_files': finished task run for task with final state: 'Success'\n",
      "[2019-12-21 16:07:51,734] INFO - prefect.TaskRunner | Task 'process_and_write': Starting task run...\n",
      "[2019-12-21 16:07:51,753] INFO - prefect.TaskRunner | Task 'process_and_write[0]': Starting task run...\n",
      "[2019-12-21 16:07:53,568] INFO - prefect.TaskRunner | Task 'process_and_write[0]': finished task run for task with final state: 'Success'\n",
      "[2019-12-21 16:07:53,578] INFO - prefect.TaskRunner | Task 'process_and_write[1]': Starting task run...\n",
      "[2019-12-21 16:07:55,107] INFO - prefect.TaskRunner | Task 'process_and_write[1]': finished task run for task with final state: 'Success'\n",
      "[2019-12-21 16:07:55,113] INFO - prefect.TaskRunner | Task 'process_and_write[2]': Starting task run...\n",
      "[2019-12-21 16:07:56,148] INFO - prefect.TaskRunner | Task 'process_and_write[2]': finished task run for task with final state: 'Success'\n",
      "[2019-12-21 16:07:56,153] INFO - prefect.TaskRunner | Task 'process_and_write[3]': Starting task run...\n",
      "[2019-12-21 16:07:56,849] INFO - prefect.TaskRunner | Task 'process_and_write[3]': finished task run for task with final state: 'Success'\n",
      "[2019-12-21 16:07:56,858] INFO - prefect.TaskRunner | Task 'process_and_write': finished task run for task with final state: 'Mapped'\n",
      "[2019-12-21 16:07:56,882] INFO - prefect.TaskRunner | Task 'combine_parquet_files': Starting task run...\n",
      "[2019-12-21 16:07:56,911] INFO - prefect.TaskRunner | Task 'combine_parquet_files': finished task run for task with final state: 'Success'\n",
      "[2019-12-21 16:07:56,914] INFO - prefect.FlowRunner | Flow run SUCCESS: all reference tasks succeeded\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "from testing import test_eq, path\n",
    "from omegaconf import OmegaConf\n",
    "import fastparquet\n",
    "\n",
    "tmp_path = \"/tmp/neuralmusic_etl\"\n",
    "\n",
    "targz_path = path(\"data/midi.tar.gz\")\n",
    "\n",
    "dot_list = [f\"tar_gz_path={targz_path}\", f\"outdir={tmp_path}\", \"partition_size=1\"]\n",
    "etl_cfg = OmegaConf.from_dotlist(dot_list)\n",
    "flow = build_etl(etl_cfg)\n",
    "\n",
    "init_stats()\n",
    "\n",
    "started_at = time.time()\n",
    "flow.run()\n",
    "\n",
    "test_eq(4, total_songs)\n",
    "test_eq(0, malformed_songs)\n",
    "test_eq(4, valid_songs)\n",
    "test_eq(4, valid_rows)\n",
    "\n",
    "\n",
    "df = fastparquet.ParquetFile(tmp_path, verify=True).to_pandas()\n",
    "test_eq(4, len(df))\n",
    "\n",
    "# TODO: figure out order!\n",
    "# test_eq([\"7.11.2\", \"7\", \"7\"], pitches[0:3])\n",
    "# test_eq([1.75, 0.5, 0.5], durations[0:3])\n",
    "# test_eq([110, 110, 110], velocities[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also accepts a path to a folder with MIDI files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-21 16:08:00,888] INFO - prefect.FlowRunner | Beginning Flow run for 'Neuralmusic Data ETL'\n",
      "[2019-12-21 16:08:00,905] INFO - prefect.FlowRunner | Starting flow run.\n",
      "[2019-12-21 16:08:00,926] INFO - prefect.TaskRunner | Task 'partition_files': Starting task run...\n",
      "[2019-12-21 16:08:00,934] INFO - prefect.Task: partition_files | Processing 4 MIDI files partitioned into groups of 1\n",
      "[2019-12-21 16:08:00,966] INFO - prefect.TaskRunner | Task 'partition_files': finished task run for task with final state: 'Success'\n",
      "[2019-12-21 16:08:00,984] INFO - prefect.TaskRunner | Task 'process_and_write': Starting task run...\n",
      "[2019-12-21 16:08:00,999] INFO - prefect.TaskRunner | Task 'process_and_write[0]': Starting task run...\n",
      "[2019-12-21 16:08:01,764] INFO - prefect.TaskRunner | Task 'process_and_write[0]': finished task run for task with final state: 'Success'\n",
      "[2019-12-21 16:08:01,772] INFO - prefect.TaskRunner | Task 'process_and_write[1]': Starting task run...\n",
      "[2019-12-21 16:08:02,979] INFO - prefect.TaskRunner | Task 'process_and_write[1]': finished task run for task with final state: 'Success'\n",
      "[2019-12-21 16:08:02,987] INFO - prefect.TaskRunner | Task 'process_and_write[2]': Starting task run...\n",
      "[2019-12-21 16:08:04,316] INFO - prefect.TaskRunner | Task 'process_and_write[2]': finished task run for task with final state: 'Success'\n",
      "[2019-12-21 16:08:04,323] INFO - prefect.TaskRunner | Task 'process_and_write[3]': Starting task run...\n",
      "[2019-12-21 16:08:05,565] INFO - prefect.TaskRunner | Task 'process_and_write[3]': finished task run for task with final state: 'Success'\n",
      "[2019-12-21 16:08:05,571] INFO - prefect.TaskRunner | Task 'process_and_write': finished task run for task with final state: 'Mapped'\n",
      "[2019-12-21 16:08:05,618] INFO - prefect.TaskRunner | Task 'combine_parquet_files': Starting task run...\n",
      "[2019-12-21 16:08:05,646] INFO - prefect.TaskRunner | Task 'combine_parquet_files': finished task run for task with final state: 'Success'\n",
      "[2019-12-21 16:08:05,648] INFO - prefect.FlowRunner | Flow run SUCCESS: all reference tasks succeeded\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "from testing import test_eq, path\n",
    "from omegaconf import OmegaConf\n",
    "import fastparquet\n",
    "\n",
    "tmp_path = \"/tmp/neuralmusic_etl\"\n",
    "\n",
    "midi_path = path(\"data\")\n",
    "\n",
    "dot_list = [f\"midi_path={midi_path}\", f\"outdir={tmp_path}\", \"partition_size=1\"]\n",
    "etl_cfg = OmegaConf.from_dotlist(dot_list)\n",
    "flow = build_etl(etl_cfg)\n",
    "\n",
    "init_stats()\n",
    "\n",
    "started_at = time.time()\n",
    "flow.run()\n",
    "\n",
    "test_eq(4, total_songs)\n",
    "test_eq(0, malformed_songs)\n",
    "test_eq(4, valid_songs)\n",
    "test_eq(4, valid_rows)\n",
    "\n",
    "\n",
    "df = fastparquet.ParquetFile(tmp_path, verify=True).to_pandas()\n",
    "test_eq(4, len(df))\n",
    "\n",
    "# TODO: figure out order!\n",
    "# test_eq([\"7.11.2\", \"7\", \"7\"], pitches[0:3])\n",
    "# test_eq([1.75, 0.5, 0.5], durations[0:3])\n",
    "# test_eq([110, 110, 110], velocities[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
