{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"# hide\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# hide\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# export\\nfrom typing import Collection\\n\\nimport numpy as np\\nfrom omegaconf import OmegaConf\\nimport torch\\nfrom torch import Tensor\\nimport torch.nn as nn\\nfrom fastai2.text.models import RNNDropout\\n\\nfrom neuralmusic.midi import Triplet\\nfrom neuralmusic.data.preprocessing import Vocab\";\n",
       "                var nbb_formatted_code = \"# export\\nfrom typing import Collection\\n\\nimport numpy as np\\nfrom omegaconf import OmegaConf\\nimport torch\\nfrom torch import Tensor\\nimport torch.nn as nn\\nfrom fastai2.text.models import RNNDropout\\n\\nfrom neuralmusic.midi import Triplet\\nfrom neuralmusic.data.preprocessing import Vocab\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "from typing import Collection\n",
    "\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from fastai2.text.models import RNNDropout\n",
    "\n",
    "from neuralmusic.midi import Triplet\n",
    "from neuralmusic.data.preprocessing import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"# default_exp model\";\n",
       "                var nbb_formatted_code = \"# default_exp model\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default_exp model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "> Learning melody and rhythm at the same time\n",
    "\n",
    "We're attempting to build a model that can effectively learn two parallel signals (pitch and duration) at the same time, with a single loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before, we'll take a piece from Fastai v1 that I couldn't find in Fastai2, the Linear Decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"# export\\n\\n\\nclass LinearDecoder(nn.Module):\\n    \\\"\\\"\\\"\\n    A Linear Decoder from fastai v1.\\n    \\\"\\\"\\\"\\n\\n    initrange = 0.1\\n\\n    def __init__(\\n        self,\\n        n_out: int,\\n        n_hid: int,\\n        output_p: float,\\n        tie_encoder=None,\\n        bias: bool = True,\\n    ):\\n        super().__init__()\\n        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\\n        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\\n        self.output_dp = RNNDropout(output_p)\\n        if bias:\\n            self.decoder.bias.data.zero_()\\n        if tie_encoder:\\n            self.decoder.weight = tie_encoder.weight\\n\\n    def forward(self, input: Tensor) -> Tensor:\\n        output = self.output_dp(input)\\n        decoded = self.decoder(\\n            output.view(output.size(0) * output.size(1), output.size(2))\\n        )\\n        return decoded\";\n",
       "                var nbb_formatted_code = \"# export\\n\\n\\nclass LinearDecoder(nn.Module):\\n    \\\"\\\"\\\"\\n    A Linear Decoder from fastai v1.\\n    \\\"\\\"\\\"\\n\\n    initrange = 0.1\\n\\n    def __init__(\\n        self,\\n        n_out: int,\\n        n_hid: int,\\n        output_p: float,\\n        tie_encoder=None,\\n        bias: bool = True,\\n    ):\\n        super().__init__()\\n        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\\n        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\\n        self.output_dp = RNNDropout(output_p)\\n        if bias:\\n            self.decoder.bias.data.zero_()\\n        if tie_encoder:\\n            self.decoder.weight = tie_encoder.weight\\n\\n    def forward(self, input: Tensor) -> Tensor:\\n        output = self.output_dp(input)\\n        decoded = self.decoder(\\n            output.view(output.size(0) * output.size(1), output.size(2))\\n        )\\n        return decoded\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class LinearDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A Linear Decoder from fastai v1.\n",
    "    \"\"\"\n",
    "\n",
    "    initrange = 0.1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_out: int,\n",
    "        n_hid: int,\n",
    "        output_p: float,\n",
    "        tie_encoder=None,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        if bias:\n",
    "            self.decoder.bias.data.zero_()\n",
    "        if tie_encoder:\n",
    "            self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        output = self.output_dp(input)\n",
    "        decoded = self.decoder(\n",
    "            output.view(output.size(0) * output.size(1), output.size(2))\n",
    "        )\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"# export\\n\\n\\nclass TheModel(nn.Module):\\n    \\\"\\\"\\\"\\n    A model that learns pitch and duration through separate RNNs, merging them at\\n    the end to 'compare notes', and outputting separate predictions for each aspect.\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        pitch_len,\\n        duration_len,\\n        kind,\\n        emb_size=1000,\\n        rnn_size=1200,\\n        rnn_layers=3,\\n        dropout=0.0,\\n    ):\\n        super().__init__()\\n        \\n        self.kind = kind # TODO: use\\n\\n        self.pitch_emb = nn.Embedding(\\n            num_embeddings=pitch_len, embedding_dim=emb_size, padding_idx=1\\n        )\\n        self.duration_emb = nn.Embedding(\\n            num_embeddings=duration_len, embedding_dim=emb_size, padding_idx=1\\n        )\\n\\n        self.hidden_size = rnn_size\\n        self.layers = rnn_layers\\n        self.pitch_rnn = nn.GRU(\\n            input_size=emb_size,\\n            hidden_size=rnn_size,\\n            num_layers=rnn_layers,\\n            batch_first=False,\\n            bidirectional=False,\\n        )\\n        self.duration_rnn = nn.GRU(\\n            input_size=emb_size,\\n            hidden_size=rnn_size,\\n            num_layers=rnn_layers,\\n            batch_first=False,\\n            bidirectional=False,\\n        )\\n\\n        self.pitch_dec = LinearDecoder(\\n            n_out=pitch_len, n_hid=rnn_size + rnn_size, output_p=dropout\\n        )\\n        self.duration_dec = LinearDecoder(\\n            n_out=duration_len, n_hid=rnn_size + rnn_size, output_p=dropout\\n        )\\n\\n        self.reset()\\n\\n    def forward(self, x):\\n        pitches, durations = x.transpose(0, 1)\\n\\n        pitch_emb = self.pitch_emb(pitches).transpose(0, 1)\\n        duration_emb = self.duration_emb(durations).transpose(0, 1)\\n\\n        if self.pitch_hid is None:\\n            self.pitch_hid = self.init_hidden(\\n                self.layers, pitches.size(0), self.hidden_size\\n            )\\n        if self.duration_hid is None:\\n            self.duration_hid = self.init_hidden(\\n                self.layers, durations.size(0), self.hidden_size\\n            )\\n\\n        pitch, self.pitch_hid = self.pitch_rnn(pitch_emb, self.pitch_hid)\\n        duration, self.duration_hid = self.duration_rnn(duration_emb, self.duration_hid)\\n\\n        together = torch.cat([pitch, duration], dim=2)\\n\\n        pitch_decoded = self.pitch_dec(together)\\n        duration_decoded = self.duration_dec(together)\\n\\n        self.pitch_hid.detach_()\\n        self.duration_hid.detach_()\\n\\n        pitch_out = pitch_decoded.view(pitches.size(0), pitches.size(1), -1)\\n        duration_out = duration_decoded.view(durations.size(0), durations.size(1), -1)\\n\\n        return pitch_out, duration_out\\n\\n    def reset(self):\\n        self.pitch_hid = None\\n        self.duration_hid = None\\n\\n    def init_hidden(self, layers, batch_size, hidden_size):\\n        weight = next(self.parameters()).data\\n        return weight.new(layers, batch_size, hidden_size).zero_()\";\n",
       "                var nbb_formatted_code = \"# export\\n\\n\\nclass TheModel(nn.Module):\\n    \\\"\\\"\\\"\\n    A model that learns pitch and duration through separate RNNs, merging them at\\n    the end to 'compare notes', and outputting separate predictions for each aspect.\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        pitch_len,\\n        duration_len,\\n        kind,\\n        emb_size=1000,\\n        rnn_size=1200,\\n        rnn_layers=3,\\n        dropout=0.0,\\n    ):\\n        super().__init__()\\n\\n        self.kind = kind  # TODO: use\\n\\n        self.pitch_emb = nn.Embedding(\\n            num_embeddings=pitch_len, embedding_dim=emb_size, padding_idx=1\\n        )\\n        self.duration_emb = nn.Embedding(\\n            num_embeddings=duration_len, embedding_dim=emb_size, padding_idx=1\\n        )\\n\\n        self.hidden_size = rnn_size\\n        self.layers = rnn_layers\\n        self.pitch_rnn = nn.GRU(\\n            input_size=emb_size,\\n            hidden_size=rnn_size,\\n            num_layers=rnn_layers,\\n            batch_first=False,\\n            bidirectional=False,\\n        )\\n        self.duration_rnn = nn.GRU(\\n            input_size=emb_size,\\n            hidden_size=rnn_size,\\n            num_layers=rnn_layers,\\n            batch_first=False,\\n            bidirectional=False,\\n        )\\n\\n        self.pitch_dec = LinearDecoder(\\n            n_out=pitch_len, n_hid=rnn_size + rnn_size, output_p=dropout\\n        )\\n        self.duration_dec = LinearDecoder(\\n            n_out=duration_len, n_hid=rnn_size + rnn_size, output_p=dropout\\n        )\\n\\n        self.reset()\\n\\n    def forward(self, x):\\n        pitches, durations = x.transpose(0, 1)\\n\\n        pitch_emb = self.pitch_emb(pitches).transpose(0, 1)\\n        duration_emb = self.duration_emb(durations).transpose(0, 1)\\n\\n        if self.pitch_hid is None:\\n            self.pitch_hid = self.init_hidden(\\n                self.layers, pitches.size(0), self.hidden_size\\n            )\\n        if self.duration_hid is None:\\n            self.duration_hid = self.init_hidden(\\n                self.layers, durations.size(0), self.hidden_size\\n            )\\n\\n        pitch, self.pitch_hid = self.pitch_rnn(pitch_emb, self.pitch_hid)\\n        duration, self.duration_hid = self.duration_rnn(duration_emb, self.duration_hid)\\n\\n        together = torch.cat([pitch, duration], dim=2)\\n\\n        pitch_decoded = self.pitch_dec(together)\\n        duration_decoded = self.duration_dec(together)\\n\\n        self.pitch_hid.detach_()\\n        self.duration_hid.detach_()\\n\\n        pitch_out = pitch_decoded.view(pitches.size(0), pitches.size(1), -1)\\n        duration_out = duration_decoded.view(durations.size(0), durations.size(1), -1)\\n\\n        return pitch_out, duration_out\\n\\n    def reset(self):\\n        self.pitch_hid = None\\n        self.duration_hid = None\\n\\n    def init_hidden(self, layers, batch_size, hidden_size):\\n        weight = next(self.parameters()).data\\n        return weight.new(layers, batch_size, hidden_size).zero_()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class TheModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model that learns pitch and duration through separate RNNs, merging them at\n",
    "    the end to 'compare notes', and outputting separate predictions for each aspect.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pitch_len,\n",
    "        duration_len,\n",
    "        kind,\n",
    "        emb_size=1000,\n",
    "        rnn_size=1200,\n",
    "        rnn_layers=3,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.kind = kind  # TODO: use\n",
    "\n",
    "        self.pitch_emb = nn.Embedding(\n",
    "            num_embeddings=pitch_len, embedding_dim=emb_size, padding_idx=1\n",
    "        )\n",
    "        self.duration_emb = nn.Embedding(\n",
    "            num_embeddings=duration_len, embedding_dim=emb_size, padding_idx=1\n",
    "        )\n",
    "\n",
    "        self.hidden_size = rnn_size\n",
    "        self.layers = rnn_layers\n",
    "        self.pitch_rnn = nn.GRU(\n",
    "            input_size=emb_size,\n",
    "            hidden_size=rnn_size,\n",
    "            num_layers=rnn_layers,\n",
    "            batch_first=False,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "        self.duration_rnn = nn.GRU(\n",
    "            input_size=emb_size,\n",
    "            hidden_size=rnn_size,\n",
    "            num_layers=rnn_layers,\n",
    "            batch_first=False,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "        self.pitch_dec = LinearDecoder(\n",
    "            n_out=pitch_len, n_hid=rnn_size + rnn_size, output_p=dropout\n",
    "        )\n",
    "        self.duration_dec = LinearDecoder(\n",
    "            n_out=duration_len, n_hid=rnn_size + rnn_size, output_p=dropout\n",
    "        )\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pitches, durations = x.transpose(0, 1)\n",
    "\n",
    "        pitch_emb = self.pitch_emb(pitches).transpose(0, 1)\n",
    "        duration_emb = self.duration_emb(durations).transpose(0, 1)\n",
    "\n",
    "        if self.pitch_hid is None:\n",
    "            self.pitch_hid = self.init_hidden(\n",
    "                self.layers, pitches.size(0), self.hidden_size\n",
    "            )\n",
    "        if self.duration_hid is None:\n",
    "            self.duration_hid = self.init_hidden(\n",
    "                self.layers, durations.size(0), self.hidden_size\n",
    "            )\n",
    "\n",
    "        pitch, self.pitch_hid = self.pitch_rnn(pitch_emb, self.pitch_hid)\n",
    "        duration, self.duration_hid = self.duration_rnn(duration_emb, self.duration_hid)\n",
    "\n",
    "        together = torch.cat([pitch, duration], dim=2)\n",
    "\n",
    "        pitch_decoded = self.pitch_dec(together)\n",
    "        duration_decoded = self.duration_dec(together)\n",
    "\n",
    "        self.pitch_hid.detach_()\n",
    "        self.duration_hid.detach_()\n",
    "\n",
    "        pitch_out = pitch_decoded.view(pitches.size(0), pitches.size(1), -1)\n",
    "        duration_out = duration_decoded.view(durations.size(0), durations.size(1), -1)\n",
    "\n",
    "        return pitch_out, duration_out\n",
    "\n",
    "    def reset(self):\n",
    "        self.pitch_hid = None\n",
    "        self.duration_hid = None\n",
    "\n",
    "    def init_hidden(self, layers, batch_size, hidden_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(layers, batch_size, hidden_size).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"# export\\n\\n\\ndef triplets_to_input(\\n    triplets: Collection[Triplet], pitch_vocab, duration_vocab\\n) -> torch.Tensor:\\n    \\\"\\\"\\\"\\n    Formats a sequence of triplets as an input to the model.\\n    \\\"\\\"\\\"\\n    return torch.tensor(\\n        [\\n            [\\n                [pitch_vocab.index(p) for (p, _, _) in triplets],\\n                [duration_vocab.index(str(d)) for (_, d, _) in triplets],\\n            ]\\n        ]\\n    )\";\n",
       "                var nbb_formatted_code = \"# export\\n\\n\\ndef triplets_to_input(\\n    triplets: Collection[Triplet], pitch_vocab, duration_vocab\\n) -> torch.Tensor:\\n    \\\"\\\"\\\"\\n    Formats a sequence of triplets as an input to the model.\\n    \\\"\\\"\\\"\\n    return torch.tensor(\\n        [\\n            [\\n                [pitch_vocab.index(p) for (p, _, _) in triplets],\\n                [duration_vocab.index(str(d)) for (_, d, _) in triplets],\\n            ]\\n        ]\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def triplets_to_input(\n",
    "    triplets: Collection[Triplet], pitch_vocab, duration_vocab\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Formats a sequence of triplets as an input to the model.\n",
    "    \"\"\"\n",
    "    return torch.tensor(\n",
    "        [\n",
    "            [\n",
    "                [pitch_vocab.index(p) for (p, _, _) in triplets],\n",
    "                [duration_vocab.index(str(d)) for (_, d, _) in triplets],\n",
    "            ]\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TheModel(\n",
       "  (pitch_emb): Embedding(56, 1000, padding_idx=1)\n",
       "  (duration_emb): Embedding(24, 1000, padding_idx=1)\n",
       "  (pitch_rnn): GRU(1000, 1200, num_layers=2)\n",
       "  (duration_rnn): GRU(1000, 1200, num_layers=2)\n",
       "  (pitch_dec): LinearDecoder(\n",
       "    (decoder): Linear(in_features=2400, out_features=56, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       "  (duration_dec): LinearDecoder(\n",
       "    (decoder): Linear(in_features=2400, out_features=24, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"# test\\nfrom fastai2.text.data import make_vocab\\n\\nfrom testing import test_eq, path\\n\\nfrom neuralmusic.midi import parse_midi_file, row_to_triplets\\nfrom neuralmusic.data.preprocessing import preprocess\\n\\nraw_df, notes = parse_midi_file(path(\\\"data/ff4-airship.mid\\\"))\\ndf, pitch_count, duration_count = preprocess(raw_df)\\n\\nsong = row_to_triplets(df, 0)\\n\\nbatch_size = 1\\nseq_len = 10\\nprompt = song[0:seq_len]\\n\\npitch_vocab = make_vocab(pitch_count, min_freq=1)\\nduration_vocab = make_vocab(duration_count, min_freq=1)\\n\\nmodel = TheModel(\\n    pitch_len=len(pitch_vocab),\\n    duration_len=len(duration_vocab),\\n    kind='dual',\\n    emb_size=1000,\\n    rnn_size=1200,\\n    rnn_layers=2,\\n)\\n\\npitch_out, duration_out = model(triplets_to_input(prompt, pitch_vocab, duration_vocab))\\n\\ntest_eq(torch.Size([batch_size, seq_len, len(pitch_vocab)]), pitch_out.shape)\\ntest_eq(torch.Size([batch_size, seq_len, len(duration_vocab)]), duration_out.shape)\\n\\nmodel\";\n",
       "                var nbb_formatted_code = \"# test\\nfrom fastai2.text.data import make_vocab\\n\\nfrom testing import test_eq, path\\n\\nfrom neuralmusic.midi import parse_midi_file, row_to_triplets\\nfrom neuralmusic.data.preprocessing import preprocess\\n\\nraw_df, notes = parse_midi_file(path(\\\"data/ff4-airship.mid\\\"))\\ndf, pitch_count, duration_count = preprocess(raw_df)\\n\\nsong = row_to_triplets(df, 0)\\n\\nbatch_size = 1\\nseq_len = 10\\nprompt = song[0:seq_len]\\n\\npitch_vocab = make_vocab(pitch_count, min_freq=1)\\nduration_vocab = make_vocab(duration_count, min_freq=1)\\n\\nmodel = TheModel(\\n    pitch_len=len(pitch_vocab),\\n    duration_len=len(duration_vocab),\\n    kind=\\\"dual\\\",\\n    emb_size=1000,\\n    rnn_size=1200,\\n    rnn_layers=2,\\n)\\n\\npitch_out, duration_out = model(triplets_to_input(prompt, pitch_vocab, duration_vocab))\\n\\ntest_eq(torch.Size([batch_size, seq_len, len(pitch_vocab)]), pitch_out.shape)\\ntest_eq(torch.Size([batch_size, seq_len, len(duration_vocab)]), duration_out.shape)\\n\\nmodel\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "from fastai2.text.data import make_vocab\n",
    "\n",
    "from testing import test_eq, path\n",
    "\n",
    "from neuralmusic.midi import parse_midi_file, row_to_triplets\n",
    "from neuralmusic.data.preprocessing import preprocess\n",
    "\n",
    "raw_df, notes = parse_midi_file(path(\"data/ff4-airship.mid\"))\n",
    "df, pitch_count, duration_count = preprocess(raw_df)\n",
    "\n",
    "song = row_to_triplets(df, 0)\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 10\n",
    "prompt = song[0:seq_len]\n",
    "\n",
    "pitch_vocab = make_vocab(pitch_count, min_freq=1)\n",
    "duration_vocab = make_vocab(duration_count, min_freq=1)\n",
    "\n",
    "model = TheModel(\n",
    "    pitch_len=len(pitch_vocab),\n",
    "    duration_len=len(duration_vocab),\n",
    "    kind=\"dual\",\n",
    "    emb_size=1000,\n",
    "    rnn_size=1200,\n",
    "    rnn_layers=2,\n",
    ")\n",
    "\n",
    "pitch_out, duration_out = model(triplets_to_input(prompt, pitch_vocab, duration_vocab))\n",
    "\n",
    "test_eq(torch.Size([batch_size, seq_len, len(pitch_vocab)]), pitch_out.shape)\n",
    "test_eq(torch.Size([batch_size, seq_len, len(duration_vocab)]), duration_out.shape)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "To predict notes from a prompt (a sequence of triplets to prime the model), we'll need a couple more functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"# export\\n\\n\\ndef choose(top_k, logits, vocab):\\n    \\\"\\\"\\\"\\n    Chooses between the top K probabilities, and returns a single random choice.\\n    \\\"\\\"\\\"\\n    last_logits = logits.squeeze(0)[-1]\\n    top_vals, top_ix = torch.topk(last_logits, k=top_k)\\n    choice = np.random.choice(top_ix.tolist())\\n    category = vocab[choice]\\n    return choice, category\\n\\n\\ndef predict(device, model, prompt, pitch_vocab, duration_vocab, top_k=5, n_notes=4):\\n    \\\"\\\"\\\"\\n    Predicts the next n notes given a model and a prompt.\\n    \\\"\\\"\\\"\\n    model.reset()\\n    model.eval()\\n    notes = []\\n    input = triplets_to_input(prompt, pitch_vocab, duration_vocab).to(device)\\n    for n in range(n_notes):\\n        pitch_out, duration_out = model(input)\\n        pitch_encoded, pitch = choose(top_k, pitch_out, pitch_vocab)\\n        duration_encoded, duration = choose(top_k, duration_out, duration_vocab)\\n        input = torch.tensor([[[pitch_encoded], [duration_encoded]]]).to(device)\\n        notes.append((pitch, duration))\\n\\n    return notes\";\n",
       "                var nbb_formatted_code = \"# export\\n\\n\\ndef choose(top_k, logits, vocab):\\n    \\\"\\\"\\\"\\n    Chooses between the top K probabilities, and returns a single random choice.\\n    \\\"\\\"\\\"\\n    last_logits = logits.squeeze(0)[-1]\\n    top_vals, top_ix = torch.topk(last_logits, k=top_k)\\n    choice = np.random.choice(top_ix.tolist())\\n    category = vocab[choice]\\n    return choice, category\\n\\n\\ndef predict(device, model, prompt, pitch_vocab, duration_vocab, top_k=5, n_notes=4):\\n    \\\"\\\"\\\"\\n    Predicts the next n notes given a model and a prompt.\\n    \\\"\\\"\\\"\\n    model.reset()\\n    model.eval()\\n    notes = []\\n    input = triplets_to_input(prompt, pitch_vocab, duration_vocab).to(device)\\n    for n in range(n_notes):\\n        pitch_out, duration_out = model(input)\\n        pitch_encoded, pitch = choose(top_k, pitch_out, pitch_vocab)\\n        duration_encoded, duration = choose(top_k, duration_out, duration_vocab)\\n        input = torch.tensor([[[pitch_encoded], [duration_encoded]]]).to(device)\\n        notes.append((pitch, duration))\\n\\n    return notes\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def choose(top_k, logits, vocab):\n",
    "    \"\"\"\n",
    "    Chooses between the top K probabilities, and returns a single random choice.\n",
    "    \"\"\"\n",
    "    last_logits = logits.squeeze(0)[-1]\n",
    "    top_vals, top_ix = torch.topk(last_logits, k=top_k)\n",
    "    choice = np.random.choice(top_ix.tolist())\n",
    "    category = vocab[choice]\n",
    "    return choice, category\n",
    "\n",
    "\n",
    "def predict(device, model, prompt, pitch_vocab, duration_vocab, top_k=5, n_notes=4):\n",
    "    \"\"\"\n",
    "    Predicts the next n notes given a model and a prompt.\n",
    "    \"\"\"\n",
    "    model.reset()\n",
    "    model.eval()\n",
    "    notes = []\n",
    "    input = triplets_to_input(prompt, pitch_vocab, duration_vocab).to(device)\n",
    "    for n in range(n_notes):\n",
    "        pitch_out, duration_out = model(input)\n",
    "        pitch_encoded, pitch = choose(top_k, pitch_out, pitch_vocab)\n",
    "        duration_encoded, duration = choose(top_k, duration_out, duration_vocab)\n",
    "        input = torch.tensor([[[pitch_encoded], [duration_encoded]]]).to(device)\n",
    "        notes.append((pitch, duration))\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxfake', 'xxfake', 52, 17)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"# test\\npredicted = predict(\\n    torch.device(\\\"cpu\\\"), model, prompt, pitch_vocab, duration_vocab, top_k=1, n_notes=5\\n)\\npitch, duration = predicted[0]\\n\\npitch, duration, pitch_vocab.index(pitch), duration_vocab.index(duration)\";\n",
       "                var nbb_formatted_code = \"# test\\npredicted = predict(\\n    torch.device(\\\"cpu\\\"), model, prompt, pitch_vocab, duration_vocab, top_k=1, n_notes=5\\n)\\npitch, duration = predicted[0]\\n\\npitch, duration, pitch_vocab.index(pitch), duration_vocab.index(duration)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "predicted = predict(\n",
    "    torch.device(\"cpu\"), model, prompt, pitch_vocab, duration_vocab, top_k=1, n_notes=5\n",
    ")\n",
    "pitch, duration = predicted[0]\n",
    "\n",
    "pitch, duration, pitch_vocab.index(pitch), duration_vocab.index(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"# export\\n\\n\\ndef get_model(cfg: OmegaConf, pitch_vocab: Vocab, duration_vocab: Vocab) -> TheModel:\\n    return TheModel(\\n        pitch_len=len(pitch_vocab),\\n        duration_len=len(duration_vocab),\\n        kind=cfg.name,\\n        emb_size=cfg.emb_size,\\n        rnn_size=cfg.rnn_size,\\n        rnn_layers=cfg.rnn_layers,\\n    )\";\n",
       "                var nbb_formatted_code = \"# export\\n\\n\\ndef get_model(cfg: OmegaConf, pitch_vocab: Vocab, duration_vocab: Vocab) -> TheModel:\\n    return TheModel(\\n        pitch_len=len(pitch_vocab),\\n        duration_len=len(duration_vocab),\\n        kind=cfg.name,\\n        emb_size=cfg.emb_size,\\n        rnn_size=cfg.rnn_size,\\n        rnn_layers=cfg.rnn_layers,\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_model(cfg: OmegaConf, pitch_vocab: Vocab, duration_vocab: Vocab) -> TheModel:\n",
    "    return TheModel(\n",
    "        pitch_len=len(pitch_vocab),\n",
    "        duration_len=len(duration_vocab),\n",
    "        kind=cfg.name,\n",
    "        emb_size=cfg.emb_size,\n",
    "        rnn_size=cfg.rnn_size,\n",
    "        rnn_layers=cfg.rnn_layers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
