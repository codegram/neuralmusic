{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# hide\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# hide\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# export\\nfrom typing import Collection, Counter\\n\\nimport fastparquet\\nimport pandas as pd\\nimport numpy as np\\nimport torch\\n\\nfrom fastai2.basics import (\\n    Chunks,\\n    ReindexCollection,\\n    round_multiple,\\n    delegates,\\n    tuplify,\\n    Tuple,\\n    IndexSplitter,\\n    DataSource,\\n    DataBunch,\\n    attrgetter,\\n    range_of,\\n    Cuda,\\n)\\nfrom fastai2.text.data import (\\n    LMTensorText,\\n    tokenize_df,\\n    BaseTokenizer,\\n    make_vocab,\\n    LMDataLoader,\\n    Numericalize,\\n)\";\n",
       "                var nbb_formatted_code = \"# export\\nfrom typing import Collection, Counter\\n\\nimport fastparquet\\nimport pandas as pd\\nimport numpy as np\\nimport torch\\n\\nfrom fastai2.basics import (\\n    Chunks,\\n    ReindexCollection,\\n    round_multiple,\\n    delegates,\\n    tuplify,\\n    Tuple,\\n    IndexSplitter,\\n    DataSource,\\n    DataBunch,\\n    attrgetter,\\n    range_of,\\n    Cuda,\\n)\\nfrom fastai2.text.data import (\\n    LMTensorText,\\n    tokenize_df,\\n    BaseTokenizer,\\n    make_vocab,\\n    LMDataLoader,\\n    Numericalize,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "from typing import Collection, Counter\n",
    "\n",
    "import fastparquet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from fastai2.basics import (\n",
    "    Chunks,\n",
    "    ReindexCollection,\n",
    "    round_multiple,\n",
    "    delegates,\n",
    "    tuplify,\n",
    "    Tuple,\n",
    "    IndexSplitter,\n",
    "    DataSource,\n",
    "    DataBunch,\n",
    "    attrgetter,\n",
    "    range_of,\n",
    "    Cuda,\n",
    ")\n",
    "from fastai2.text.data import (\n",
    "    LMTensorText,\n",
    "    tokenize_df,\n",
    "    BaseTokenizer,\n",
    "    make_vocab,\n",
    "    LMDataLoader,\n",
    "    Numericalize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# default_exp data.preprocessing\";\n",
       "                var nbb_formatted_code = \"# default_exp data.preprocessing\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default_exp data.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "\n",
    "> From raw parquet to an ML-suitable, clean dataset\n",
    "\n",
    "Once the ETL runs, we'll have a bunch of parquet files representing MIDI songs. However, there is still a little work to do to turn those into a format that is suitable for training a model.\n",
    "\n",
    "Let's go through the steps we'll need to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading raw parquet files into raw dataframes\n",
    "\n",
    "First order of business is turning the output of the data ETL into a dataframe ready for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# export\\n\\n\\ndef read_parquet(path: str) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Reads a multi-file parquet at `path`, returning a dataframe of three columns.\\n    \\\"\\\"\\\"\\n    df = fastparquet.ParquetFile(path, verify=True).to_pandas()\\n    return df\";\n",
       "                var nbb_formatted_code = \"# export\\n\\n\\ndef read_parquet(path: str) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Reads a multi-file parquet at `path`, returning a dataframe of three columns.\\n    \\\"\\\"\\\"\\n    df = fastparquet.ParquetFile(path, verify=True).to_pandas()\\n    return df\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def read_parquet(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a multi-file parquet at `path`, returning a dataframe of three columns.\n",
    "    \"\"\"\n",
    "    df = fastparquet.ParquetFile(path, verify=True).to_pandas()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to tokenize the contents, and get ahold of the counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# export\\n\\n\\ndef preprocess(df: pd.DataFrame) -> (pd.DataFrame, Counter[str], Counter[str]):\\n    \\\"\\\"\\\"\\n    Tokenizes pitches and durations and returns a dataframe\\n    \\\"\\\"\\\"\\n    df.pitches = df.pitches.apply(lambda x: \\\" \\\".join(map(str, x)))\\n    df.durations = df.durations.apply(lambda x: \\\" \\\".join(map(str, x)))\\n    df = df[df[\\\"pitches\\\"].apply(lambda x: len(x) != 0)].reset_index()\\n    df_tok, pitch_count = tokenize_df(df, \\\"pitches\\\", rules=[], tok_func=BaseTokenizer)\\n    df_tok[\\\"pitches\\\"] = df_tok[\\\"text\\\"]\\n    df_tok, duration_count = tokenize_df(\\n        df_tok, \\\"durations\\\", rules=[], tok_func=BaseTokenizer\\n    )\\n    df_tok[\\\"durations\\\"] = df_tok[\\\"text\\\"]\\n    df_tok = df_tok.drop(\\\"text\\\", axis=1).drop(\\\"index\\\", axis=1)\\n\\n    return df_tok, pitch_count, duration_count\";\n",
       "                var nbb_formatted_code = \"# export\\n\\n\\ndef preprocess(df: pd.DataFrame) -> (pd.DataFrame, Counter[str], Counter[str]):\\n    \\\"\\\"\\\"\\n    Tokenizes pitches and durations and returns a dataframe\\n    \\\"\\\"\\\"\\n    df.pitches = df.pitches.apply(lambda x: \\\" \\\".join(map(str, x)))\\n    df.durations = df.durations.apply(lambda x: \\\" \\\".join(map(str, x)))\\n    df = df[df[\\\"pitches\\\"].apply(lambda x: len(x) != 0)].reset_index()\\n    df_tok, pitch_count = tokenize_df(df, \\\"pitches\\\", rules=[], tok_func=BaseTokenizer)\\n    df_tok[\\\"pitches\\\"] = df_tok[\\\"text\\\"]\\n    df_tok, duration_count = tokenize_df(\\n        df_tok, \\\"durations\\\", rules=[], tok_func=BaseTokenizer\\n    )\\n    df_tok[\\\"durations\\\"] = df_tok[\\\"text\\\"]\\n    df_tok = df_tok.drop(\\\"text\\\", axis=1).drop(\\\"index\\\", axis=1)\\n\\n    return df_tok, pitch_count, duration_count\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def preprocess(df: pd.DataFrame) -> (pd.DataFrame, Counter[str], Counter[str]):\n",
    "    \"\"\"\n",
    "    Tokenizes pitches and durations and returns a dataframe\n",
    "    \"\"\"\n",
    "    df.pitches = df.pitches.apply(lambda x: \" \".join(map(str, x)))\n",
    "    df.durations = df.durations.apply(lambda x: \" \".join(map(str, x)))\n",
    "    df = df[df[\"pitches\"].apply(lambda x: len(x) != 0)].reset_index()\n",
    "    df_tok, pitch_count = tokenize_df(df, \"pitches\", rules=[], tok_func=BaseTokenizer)\n",
    "    df_tok[\"pitches\"] = df_tok[\"text\"]\n",
    "    df_tok, duration_count = tokenize_df(\n",
    "        df_tok, \"durations\", rules=[], tok_func=BaseTokenizer\n",
    "    )\n",
    "    df_tok[\"durations\"] = df_tok[\"text\"]\n",
    "    df_tok = df_tok.drop(\"text\", axis=1).drop(\"index\", axis=1)\n",
    "\n",
    "    return df_tok, pitch_count, duration_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# test\\nfrom testing import test_eq, path\\n\\nfrom neuralmusic.midi import parse_midi_file\\n\\ndf, _ = parse_midi_file(path(\\\"data/ff4-airship.mid\\\"))\\ndf_tok, pitch_count, duration_count = preprocess(df)\\n\\ntest_eq([\\\"7.11.2\\\", \\\"7\\\", \\\"7\\\"], list(df_tok[\\\"pitches\\\"][0][0:3]))\\ntest_eq([\\\"1.75\\\", \\\"0.5\\\", \\\"0.5\\\"], list(df_tok[\\\"durations\\\"][0][0:3]))\\ntest_eq([110, 110, 110], list(df_tok[\\\"velocities\\\"][0][0:3]))\\n\\ntest_eq(43, len(pitch_count))\\ntest_eq(8, len(duration_count))\";\n",
       "                var nbb_formatted_code = \"# test\\nfrom testing import test_eq, path\\n\\nfrom neuralmusic.midi import parse_midi_file\\n\\ndf, _ = parse_midi_file(path(\\\"data/ff4-airship.mid\\\"))\\ndf_tok, pitch_count, duration_count = preprocess(df)\\n\\ntest_eq([\\\"7.11.2\\\", \\\"7\\\", \\\"7\\\"], list(df_tok[\\\"pitches\\\"][0][0:3]))\\ntest_eq([\\\"1.75\\\", \\\"0.5\\\", \\\"0.5\\\"], list(df_tok[\\\"durations\\\"][0][0:3]))\\ntest_eq([110, 110, 110], list(df_tok[\\\"velocities\\\"][0][0:3]))\\n\\ntest_eq(43, len(pitch_count))\\ntest_eq(8, len(duration_count))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "from testing import test_eq, path\n",
    "\n",
    "from neuralmusic.midi import parse_midi_file\n",
    "\n",
    "df, _ = parse_midi_file(path(\"data/ff4-airship.mid\"))\n",
    "df_tok, pitch_count, duration_count = preprocess(df)\n",
    "\n",
    "test_eq([\"7.11.2\", \"7\", \"7\"], list(df_tok[\"pitches\"][0][0:3]))\n",
    "test_eq([\"1.75\", \"0.5\", \"0.5\"], list(df_tok[\"durations\"][0][0:3]))\n",
    "test_eq([110, 110, 110], list(df_tok[\"velocities\"][0][0:3]))\n",
    "\n",
    "test_eq(43, len(pitch_count))\n",
    "test_eq(8, len(duration_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "\n",
    "When constructing our data source, we'll build some transforms to first get tuples of values at a time (pitches and durations), and numericalize them in parallel as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"# export\\nVocab = Collection[str]\\n\\n\\ndef to_dual(fields):\\n    \\\"\\\"\\\"\\n    Returns a transform that will extract `fields` from a Series in the form of fastai Tuples.\\n    \\\"\\\"\\\"\\n    getters = [attrgetter(field) for field in fields]\\n\\n    def _inner(series: pd.Series):\\n        return Tuple(tuplify([getter(series) for getter in getters]))\\n\\n    return _inner\\n\\n\\ndef dual_numericalize(vocabs: Collection[Vocab]):\\n    \\\"\\\"\\\"\\n    Returns a transform that will numericalize each side of the tuple constructing a separate\\n    vocabulary for each side.\\n    \\\"\\\"\\\"\\n    processors = [Numericalize(vocab) for vocab in vocabs]\\n\\n    def _inner(values):\\n        return [proc(val) for (proc, val) in zip(processors, values)]\\n\\n    return _inner\";\n",
       "                var nbb_formatted_code = \"# export\\nVocab = Collection[str]\\n\\n\\ndef to_dual(fields):\\n    \\\"\\\"\\\"\\n    Returns a transform that will extract `fields` from a Series in the form of fastai Tuples.\\n    \\\"\\\"\\\"\\n    getters = [attrgetter(field) for field in fields]\\n\\n    def _inner(series: pd.Series):\\n        return Tuple(tuplify([getter(series) for getter in getters]))\\n\\n    return _inner\\n\\n\\ndef dual_numericalize(vocabs: Collection[Vocab]):\\n    \\\"\\\"\\\"\\n    Returns a transform that will numericalize each side of the tuple constructing a separate\\n    vocabulary for each side.\\n    \\\"\\\"\\\"\\n    processors = [Numericalize(vocab) for vocab in vocabs]\\n\\n    def _inner(values):\\n        return [proc(val) for (proc, val) in zip(processors, values)]\\n\\n    return _inner\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "Vocab = Collection[str]\n",
    "\n",
    "\n",
    "def to_dual(fields):\n",
    "    \"\"\"\n",
    "    Returns a transform that will extract `fields` from a Series in the form of fastai Tuples.\n",
    "    \"\"\"\n",
    "    getters = [attrgetter(field) for field in fields]\n",
    "\n",
    "    def _inner(series: pd.Series):\n",
    "        return Tuple(tuplify([getter(series) for getter in getters]))\n",
    "\n",
    "    return _inner\n",
    "\n",
    "\n",
    "def dual_numericalize(vocabs: Collection[Vocab]):\n",
    "    \"\"\"\n",
    "    Returns a transform that will numericalize each side of the tuple constructing a separate\n",
    "    vocabulary for each side.\n",
    "    \"\"\"\n",
    "    processors = [Numericalize(vocab) for vocab in vocabs]\n",
    "\n",
    "    def _inner(values):\n",
    "        return [proc(val) for (proc, val) in zip(processors, values)]\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to make a splitter that will separate our rows according to a certain ratio, by default 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# export\\n\\n\\ndef make_splitter(df: pd.DataFrame, split: float = 0.2) -> IndexSplitter:\\n    \\\"\\\"\\\"\\n    Returns a splitter that acts on indices on a dataframe. By default it reserves 20% of the\\n    data for validation.\\n    \\\"\\\"\\\"\\n    rows = len(df)\\n    indices = np.array(range(rows))\\n    np.random.shuffle(indices)\\n    _, valid_idx = (indices[int(0.2 * rows) :], indices[: int(0.2 * rows)])\\n    return IndexSplitter(valid_idx)\";\n",
       "                var nbb_formatted_code = \"# export\\n\\n\\ndef make_splitter(df: pd.DataFrame, split: float = 0.2) -> IndexSplitter:\\n    \\\"\\\"\\\"\\n    Returns a splitter that acts on indices on a dataframe. By default it reserves 20% of the\\n    data for validation.\\n    \\\"\\\"\\\"\\n    rows = len(df)\\n    indices = np.array(range(rows))\\n    np.random.shuffle(indices)\\n    _, valid_idx = (indices[int(0.2 * rows) :], indices[: int(0.2 * rows)])\\n    return IndexSplitter(valid_idx)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def make_splitter(df: pd.DataFrame, split: float = 0.2) -> IndexSplitter:\n",
    "    \"\"\"\n",
    "    Returns a splitter that acts on indices on a dataframe. By default it reserves 20% of the\n",
    "    data for validation.\n",
    "    \"\"\"\n",
    "    rows = len(df)\n",
    "    indices = np.array(range(rows))\n",
    "    np.random.shuffle(indices)\n",
    "    _, valid_idx = (indices[int(0.2 * rows) :], indices[: int(0.2 * rows)])\n",
    "    return IndexSplitter(valid_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "We need a slightly custom DataLoader that instead of loading single sequences of tokens like in traditional language models, loads tuples of sequences (in our case, a sequence of pitches and a sequence of durations), all at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"# export\\n@delegates()\\nclass DualLMDataLoader(LMDataLoader):\\n    \\\"\\\"\\\"\\n    A Language Model data loader that loads tuples of 2 sequences instead of single sequences.\\n    It's used to load pitches and durations at the same time.\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self, dataset, lens=None, cache=2, bs=64, seq_len=72, num_workers=0, **kwargs\\n    ):\\n        super().__init__(dataset=dataset, bs=bs, num_workers=num_workers, **kwargs)\\n        self.items = ReindexCollection(\\n            [(o[0] if isinstance(o, tuple) else o) for o in dataset], cache=cache\\n        )\\n        self.seq_len = seq_len\\n        if lens is None:\\n            lens = [len(o[0]) for o in self.items]\\n        self.lens = ReindexCollection(lens, idxs=self.items.idxs)\\n        # The \\\"-1\\\" is to allow for final label\\n        self.m = round_multiple(sum(lens) - 1, bs * seq_len, round_down=True)\\n        self.n = self.m // (seq_len)\\n        self.spb = self.n // bs\\n        self.make_chunks()\\n\\n    def make_chunks(self):\\n        self.a_chunks = Chunks(list(map(lambda x: x[0], self.items)), self.lens)\\n        self.b_chunks = Chunks(list(map(lambda x: x[1], self.items)), self.lens)\\n\\n    def create_item(self, seq):\\n        if seq >= self.n:\\n            raise IndexError\\n        st = ((seq % self.bs) * self.spb + (seq // self.bs)) * self.seq_len\\n        txt_a = self.a_chunks[st : st + self.seq_len + 1]\\n        txt_b = self.b_chunks[st : st + self.seq_len + 1]\\n        x1 = LMTensorText(txt_a[:-1]).unsqueeze(-2)\\n        x2 = LMTensorText(txt_b[:-1]).unsqueeze(-2)\\n        y1 = txt_a[1:].unsqueeze(-2)\\n        y2 = txt_b[1:].unsqueeze(-2)\\n        return torch.cat([x1, x2]), torch.cat([y1, y2])\";\n",
       "                var nbb_formatted_code = \"# export\\n@delegates()\\nclass DualLMDataLoader(LMDataLoader):\\n    \\\"\\\"\\\"\\n    A Language Model data loader that loads tuples of 2 sequences instead of single sequences.\\n    It's used to load pitches and durations at the same time.\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self, dataset, lens=None, cache=2, bs=64, seq_len=72, num_workers=0, **kwargs\\n    ):\\n        super().__init__(dataset=dataset, bs=bs, num_workers=num_workers, **kwargs)\\n        self.items = ReindexCollection(\\n            [(o[0] if isinstance(o, tuple) else o) for o in dataset], cache=cache\\n        )\\n        self.seq_len = seq_len\\n        if lens is None:\\n            lens = [len(o[0]) for o in self.items]\\n        self.lens = ReindexCollection(lens, idxs=self.items.idxs)\\n        # The \\\"-1\\\" is to allow for final label\\n        self.m = round_multiple(sum(lens) - 1, bs * seq_len, round_down=True)\\n        self.n = self.m // (seq_len)\\n        self.spb = self.n // bs\\n        self.make_chunks()\\n\\n    def make_chunks(self):\\n        self.a_chunks = Chunks(list(map(lambda x: x[0], self.items)), self.lens)\\n        self.b_chunks = Chunks(list(map(lambda x: x[1], self.items)), self.lens)\\n\\n    def create_item(self, seq):\\n        if seq >= self.n:\\n            raise IndexError\\n        st = ((seq % self.bs) * self.spb + (seq // self.bs)) * self.seq_len\\n        txt_a = self.a_chunks[st : st + self.seq_len + 1]\\n        txt_b = self.b_chunks[st : st + self.seq_len + 1]\\n        x1 = LMTensorText(txt_a[:-1]).unsqueeze(-2)\\n        x2 = LMTensorText(txt_b[:-1]).unsqueeze(-2)\\n        y1 = txt_a[1:].unsqueeze(-2)\\n        y2 = txt_b[1:].unsqueeze(-2)\\n        return torch.cat([x1, x2]), torch.cat([y1, y2])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@delegates()\n",
    "class DualLMDataLoader(LMDataLoader):\n",
    "    \"\"\"\n",
    "    A Language Model data loader that loads tuples of 2 sequences instead of single sequences.\n",
    "    It's used to load pitches and durations at the same time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataset, lens=None, cache=2, bs=64, seq_len=72, num_workers=0, **kwargs\n",
    "    ):\n",
    "        super().__init__(dataset=dataset, bs=bs, num_workers=num_workers, **kwargs)\n",
    "        self.items = ReindexCollection(\n",
    "            [(o[0] if isinstance(o, tuple) else o) for o in dataset], cache=cache\n",
    "        )\n",
    "        self.seq_len = seq_len\n",
    "        if lens is None:\n",
    "            lens = [len(o[0]) for o in self.items]\n",
    "        self.lens = ReindexCollection(lens, idxs=self.items.idxs)\n",
    "        # The \"-1\" is to allow for final label\n",
    "        self.m = round_multiple(sum(lens) - 1, bs * seq_len, round_down=True)\n",
    "        self.n = self.m // (seq_len)\n",
    "        self.spb = self.n // bs\n",
    "        self.make_chunks()\n",
    "\n",
    "    def make_chunks(self):\n",
    "        self.a_chunks = Chunks(list(map(lambda x: x[0], self.items)), self.lens)\n",
    "        self.b_chunks = Chunks(list(map(lambda x: x[1], self.items)), self.lens)\n",
    "\n",
    "    def create_item(self, seq):\n",
    "        if seq >= self.n:\n",
    "            raise IndexError\n",
    "        st = ((seq % self.bs) * self.spb + (seq // self.bs)) * self.seq_len\n",
    "        txt_a = self.a_chunks[st : st + self.seq_len + 1]\n",
    "        txt_b = self.b_chunks[st : st + self.seq_len + 1]\n",
    "        x1 = LMTensorText(txt_a[:-1]).unsqueeze(-2)\n",
    "        x2 = LMTensorText(txt_b[:-1]).unsqueeze(-2)\n",
    "        y1 = txt_a[1:].unsqueeze(-2)\n",
    "        y2 = txt_b[1:].unsqueeze(-2)\n",
    "        return torch.cat([x1, x2]), torch.cat([y1, y2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we bring everything together by creating a DataSource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# export\\n\\n\\ndef data_source(\\n    df: pd.DataFrame,\\n    pitch_vocab: Vocab,\\n    duration_vocab: Vocab,\\n    split: float = 0.2,\\n    dl_type=DualLMDataLoader,\\n) -> DataSource:\\n    \\\"\\\"\\\"\\n    Creates a DataSource ready to become a databunch.\\n    \\\"\\\"\\\"\\n    splitter = make_splitter(df, split=split)\\n    return DataSource(\\n        df,\\n        [\\n            [\\n                to_dual([\\\"pitches\\\", \\\"durations\\\"]),\\n                dual_numericalize([pitch_vocab, duration_vocab]),\\n                Cuda(),\\n            ]\\n        ],\\n        splits=splitter(range_of((df))),\\n        dl_type=dl_type,\\n    )\";\n",
       "                var nbb_formatted_code = \"# export\\n\\n\\ndef data_source(\\n    df: pd.DataFrame,\\n    pitch_vocab: Vocab,\\n    duration_vocab: Vocab,\\n    split: float = 0.2,\\n    dl_type=DualLMDataLoader,\\n) -> DataSource:\\n    \\\"\\\"\\\"\\n    Creates a DataSource ready to become a databunch.\\n    \\\"\\\"\\\"\\n    splitter = make_splitter(df, split=split)\\n    return DataSource(\\n        df,\\n        [\\n            [\\n                to_dual([\\\"pitches\\\", \\\"durations\\\"]),\\n                dual_numericalize([pitch_vocab, duration_vocab]),\\n                Cuda(),\\n            ]\\n        ],\\n        splits=splitter(range_of((df))),\\n        dl_type=dl_type,\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def data_source(\n",
    "    df: pd.DataFrame,\n",
    "    pitch_vocab: Vocab,\n",
    "    duration_vocab: Vocab,\n",
    "    split: float = 0.2,\n",
    "    dl_type=DualLMDataLoader,\n",
    ") -> DataSource:\n",
    "    \"\"\"\n",
    "    Creates a DataSource ready to become a databunch.\n",
    "    \"\"\"\n",
    "    splitter = make_splitter(df, split=split)\n",
    "    return DataSource(\n",
    "        df,\n",
    "        [\n",
    "            [\n",
    "                to_dual([\"pitches\", \"durations\"]),\n",
    "                dual_numericalize([pitch_vocab, duration_vocab]),\n",
    "                Cuda(),\n",
    "            ]\n",
    "        ],\n",
    "        splits=splitter(range_of((df))),\n",
    "        dl_type=dl_type,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing everything together\n",
    "\n",
    "Now we can obtain a DataBunch ready for training from a bunch of parquet files just like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"# export\\n\\n\\ndef process(\\n    path: str, batch_size: int, seq_len: int, validation_split: float = 0.2\\n) -> (DataBunch, Vocab, Vocab):\\n    \\\"\\\"\\\"\\n    Turn raw parquet files into a DataBunch ready for training.\\n    \\\"\\\"\\\"\\n    raw_df = read_parquet(path)\\n    df, pitch_count, duration_count = preprocess(raw_df)\\n    pitch_vocab = make_vocab(pitch_count, min_count=1)\\n    duration_vocab = make_vocab(duration_count, min_count=1)\\n    dsrc = data_source(df, pitch_vocab, duration_vocab, split=validation_split)\\n    return (\\n        dsrc.databunch(bs=batch_size, seq_len=seq_len, after_batch=Cuda()),\\n        pitch_vocab,\\n        duration_vocab,\\n    )\";\n",
       "                var nbb_formatted_code = \"# export\\n\\n\\ndef process(\\n    path: str, batch_size: int, seq_len: int, validation_split: float = 0.2\\n) -> (DataBunch, Vocab, Vocab):\\n    \\\"\\\"\\\"\\n    Turn raw parquet files into a DataBunch ready for training.\\n    \\\"\\\"\\\"\\n    raw_df = read_parquet(path)\\n    df, pitch_count, duration_count = preprocess(raw_df)\\n    pitch_vocab = make_vocab(pitch_count, min_count=1)\\n    duration_vocab = make_vocab(duration_count, min_count=1)\\n    dsrc = data_source(df, pitch_vocab, duration_vocab, split=validation_split)\\n    return (\\n        dsrc.databunch(bs=batch_size, seq_len=seq_len, after_batch=Cuda()),\\n        pitch_vocab,\\n        duration_vocab,\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def process(\n",
    "    path: str, batch_size: int, seq_len: int, validation_split: float = 0.2\n",
    ") -> (DataBunch, Vocab, Vocab):\n",
    "    \"\"\"\n",
    "    Turn raw parquet files into a DataBunch ready for training.\n",
    "    \"\"\"\n",
    "    raw_df = read_parquet(path)\n",
    "    df, pitch_count, duration_count = preprocess(raw_df)\n",
    "    pitch_vocab = make_vocab(pitch_count, min_freq=1)\n",
    "    duration_vocab = make_vocab(duration_count, min_freq=1)\n",
    "    batch_size = batch_size if batch_size < len(df) else len(df)\n",
    "    dsrc = data_source(df, pitch_vocab, duration_vocab, split=validation_split)\n",
    "    return (\n",
    "        dsrc.databunch(bs=batch_size, seq_len=seq_len, after_batch=Cuda()),\n",
    "        pitch_vocab,\n",
    "        duration_vocab,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
