#AUTOGENERATED! DO NOT EDIT! File to edit: dev/03_data.preprocessing.ipynb (unless otherwise specified).

__all__ = ['read_parquet', 'preprocess', 'to_dual', 'dual_numericalize', 'Vocab', 'make_splitter', 'DualLMDataLoader',
           'data_source', 'process']

#Cell
from typing import Collection, Counter
from functools import partial

import fastparquet
import pandas as pd
import numpy as np
import torch

from fastai2.basics import (
    Chunks,
    ReindexCollection,
    round_multiple,
    delegates,
    tuplify,
    Tuple,
    IndexSplitter,
    DataSource,
    DataBunch,
    attrgetter,
    range_of,
    Cuda,
)
from fastai2.text.data import (
    LMTensorText,
    tokenize_df,
    BaseTokenizer,
    make_vocab,
    LMDataLoader,
    Numericalize,
)

#Cell


def read_parquet(path: str) -> pd.DataFrame:
    """
    Reads a multi-file parquet at `path`, returning a dataframe of three columns.
    """
    df = fastparquet.ParquetFile(path, verify=True).to_pandas()
    return df

#Cell


def preprocess(df: pd.DataFrame) -> (pd.DataFrame, Counter[str], Counter[str]):
    """
    Tokenizes pitches and durations and returns a dataframe
    """
    df.pitches = df.pitches.apply(lambda x: " ".join(map(str, x)))
    df.durations = df.durations.apply(lambda x: "|".join(map(str, x)))
    df = df[df["pitches"].apply(lambda x: len(x) != 0)].reset_index()
    df_tok, pitch_count = tokenize_df(df, "pitches", rules=[], tok_func=BaseTokenizer)
    df_tok["pitches"] = df_tok["text"]
    df_tok, duration_count = tokenize_df(
        df_tok, "durations", rules=[], tok_func=partial(BaseTokenizer, split_char="|")
    )
    df_tok["durations"] = df_tok["text"]
    df_tok = df_tok.drop("text", axis=1).drop("index", axis=1)

    return df_tok, pitch_count, duration_count

#Cell


Vocab = Collection[str]


def to_dual(fields):
    """
    Returns a transform that will extract `fields` from a Series in the form of fastai Tuples.
    """
    getters = [attrgetter(field) for field in fields]

    def _inner(series: pd.Series):
        return Tuple(tuplify([getter(series) for getter in getters]))

    return _inner


def dual_numericalize(vocabs: Collection[Vocab]):
    """
    Returns a transform that will numericalize each side of the tuple constructing a separate
    vocabulary for each side.
    """
    processors = [Numericalize(vocab) for vocab in vocabs]

    def _inner(values):
        return [proc(val) for (proc, val) in zip(processors, values)]

    return _inner

#Cell


def make_splitter(df: pd.DataFrame, split: float = 0.2) -> IndexSplitter:
    """
    Returns a splitter that acts on indices on a dataframe. By default it reserves 20% of the
    data for validation.
    """
    rows = len(df)
    indices = np.array(range(rows))
    np.random.shuffle(indices)
    _, valid_idx = (indices[int(0.2 * rows) :], indices[: int(0.2 * rows)])
    return IndexSplitter(valid_idx)

#Cell
@delegates()
class DualLMDataLoader(LMDataLoader):
    """
    A Language Model data loader that loads tuples of 2 sequences instead of single sequences.
    It's used to load pitches and durations at the same time.
    """

    def __init__(
        self, dataset, lens=None, cache=2, bs=64, seq_len=72, num_workers=0, **kwargs
    ):
        super().__init__(dataset=dataset, bs=bs, num_workers=num_workers, **kwargs)
        self.items = ReindexCollection(
            [(o[0] if isinstance(o, tuple) else o) for o in dataset], cache=cache
        )
        self.seq_len = seq_len
        if lens is None:
            lens = [len(o[0]) for o in self.items]
        self.lens = ReindexCollection(lens, idxs=self.items.idxs)
        # The "-1" is to allow for final label
        self.m = round_multiple(sum(lens) - 1, bs * seq_len, round_down=True)
        self.n = self.m // (seq_len)
        self.spb = self.n // bs
        self.make_chunks()

    def make_chunks(self):
        self.a_chunks = Chunks(list(map(lambda x: x[0], self.items)), self.lens)
        self.b_chunks = Chunks(list(map(lambda x: x[1], self.items)), self.lens)

    def create_item(self, seq):
        if seq >= self.n:
            raise IndexError
        st = ((seq % self.bs) * self.spb + (seq // self.bs)) * self.seq_len
        txt_a = self.a_chunks[st : st + self.seq_len + 1]
        txt_b = self.b_chunks[st : st + self.seq_len + 1]
        x1 = LMTensorText(txt_a[:-1]).unsqueeze(-2)
        x2 = LMTensorText(txt_b[:-1]).unsqueeze(-2)
        y1 = txt_a[1:].unsqueeze(-2)
        y2 = txt_b[1:].unsqueeze(-2)
        return torch.cat([x1, x2]), torch.cat([y1, y2])

#Cell


def data_source(
    df: pd.DataFrame,
    pitch_vocab: Vocab,
    duration_vocab: Vocab,
    split: float = 0.2,
    dl_type=DualLMDataLoader,
) -> DataSource:
    """
    Creates a DataSource ready to become a databunch.
    """
    splitter = make_splitter(df, split=split)
    return DataSource(
        df,
        [
            [
                to_dual(["pitches", "durations"]),
                dual_numericalize([pitch_vocab, duration_vocab]),
                Cuda(),
            ]
        ],
        splits=splitter(range_of((df))),
        dl_type=dl_type,
    )

#Cell


def process(
    path: str, batch_size: int, seq_len: int, validation_split: float = 0.2
) -> (DataBunch, Vocab, Vocab):
    """
    Turn raw parquet files into a DataBunch ready for training.
    """
    raw_df = read_parquet(path)
    df, pitch_count, duration_count = preprocess(raw_df)
    pitch_vocab = make_vocab(pitch_count, min_freq=1)
    duration_vocab = make_vocab(duration_count, min_freq=1)
    batch_size = batch_size if batch_size < len(df) else len(df)
    dsrc = data_source(df, pitch_vocab, duration_vocab, split=validation_split)
    return (
        dsrc.databunch(bs=batch_size, seq_len=seq_len, after_batch=Cuda()),
        pitch_vocab,
        duration_vocab,
    )