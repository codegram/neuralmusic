#AUTOGENERATED! DO NOT EDIT! File to edit: dev/04_model.ipynb (unless otherwise specified).

__all__ = ['LinearDecoder', 'TheModel', 'triplets_to_input', 'choose', 'predict']

#Cell
from typing import Collection

import numpy as np
import torch
from torch import Tensor
import torch.nn as nn
from fastai2.text.models import RNNDropout

from .midi import Triplet

#Cell


class LinearDecoder(nn.Module):
    """
    A Linear Decoder from fastai v1.
    """

    initrange = 0.1

    def __init__(
        self,
        n_out: int,
        n_hid: int,
        output_p: float,
        tie_encoder=None,
        bias: bool = True,
    ):
        super().__init__()
        self.decoder = nn.Linear(n_hid, n_out, bias=bias)
        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)
        self.output_dp = RNNDropout(output_p)
        if bias:
            self.decoder.bias.data.zero_()
        if tie_encoder:
            self.decoder.weight = tie_encoder.weight

    def forward(self, input: Tensor) -> Tensor:
        output = self.output_dp(input)
        decoded = self.decoder(
            output.view(output.size(0) * output.size(1), output.size(2))
        )
        return decoded

#Cell


class TheModel(nn.Module):
    """
    A model that learns pitch and duration through separate RNNs, merging them at
    the end to 'compare notes', and outputting separate predictions for each aspect.
    """

    def __init__(
        self,
        pitch_len,
        duration_len,
        emb_size=1000,
        rnn_size=1200,
        rnn_layers=3,
        dropout=0.0,
    ):
        super().__init__()

        self.pitch_emb = nn.Embedding(
            num_embeddings=pitch_len, embedding_dim=emb_size, padding_idx=1
        )
        self.duration_emb = nn.Embedding(
            num_embeddings=duration_len, embedding_dim=emb_size, padding_idx=1
        )

        self.hidden_size = rnn_size
        self.layers = rnn_layers
        self.pitch_rnn = nn.GRU(
            input_size=emb_size,
            hidden_size=rnn_size,
            num_layers=rnn_layers,
            batch_first=False,
            bidirectional=False,
        )
        self.duration_rnn = nn.GRU(
            input_size=emb_size,
            hidden_size=rnn_size,
            num_layers=rnn_layers,
            batch_first=False,
            bidirectional=False,
        )

        self.pitch_dec = LinearDecoder(
            n_out=pitch_len, n_hid=rnn_size + rnn_size, output_p=dropout
        )
        self.duration_dec = LinearDecoder(
            n_out=duration_len, n_hid=rnn_size + rnn_size, output_p=dropout
        )

        self.reset()

    def forward(self, x):
        pitches, durations = x.transpose(0, 1)

        pitch_emb = self.pitch_emb(pitches).transpose(0, 1)
        duration_emb = self.duration_emb(durations).transpose(0, 1)

        if self.pitch_hid is None:
            self.pitch_hid = self.init_hidden(
                self.layers, pitches.size(0), self.hidden_size
            )
        if self.duration_hid is None:
            self.duration_hid = self.init_hidden(
                self.layers, durations.size(0), self.hidden_size
            )

        pitch, self.pitch_hid = self.pitch_rnn(pitch_emb, self.pitch_hid)
        duration, self.duration_hid = self.duration_rnn(duration_emb, self.duration_hid)

        together = torch.cat([pitch, duration], dim=2)

        pitch_decoded = self.pitch_dec(together)
        duration_decoded = self.duration_dec(together)

        self.pitch_hid.detach_()
        self.duration_hid.detach_()

        pitch_out = pitch_decoded.view(pitches.size(0), pitches.size(1), -1)
        duration_out = duration_decoded.view(durations.size(0), durations.size(1), -1)

        return pitch_out, duration_out

    def reset(self):
        self.pitch_hid = None
        self.duration_hid = None

    def init_hidden(self, layers, batch_size, hidden_size):
        weight = next(self.parameters()).data
        return weight.new(layers, batch_size, hidden_size).zero_()

#Cell


def triplets_to_input(
    triplets: Collection[Triplet], pitch_vocab, duration_vocab
) -> torch.Tensor:
    """
    Formats a sequence of triplets as an input to the model.
    """
    return torch.tensor(
        [
            [
                [pitch_vocab.index(p) for (p, _, _) in triplets],
                [duration_vocab.index(str(d)) for (_, d, _) in triplets],
            ]
        ]
    )

#Cell


def choose(top_k, logits, vocab):
    """
    Chooses between the top K probabilities, and returns a single random choice.
    """
    last_logits = logits.squeeze(0)[-1]
    top_vals, top_ix = torch.topk(last_logits, k=top_k)
    choice = np.random.choice(top_ix.tolist())
    category = vocab[choice]
    return choice, category


def predict(device, model, prompt, pitch_vocab, duration_vocab, top_k=5, n_notes=4):
    """
    Predicts the next n notes given a model and a prompt.
    """
    model.reset()
    model.eval()
    notes = []
    input = triplets_to_input(prompt, pitch_vocab, duration_vocab).to(device)
    for n in range(n_notes):
        pitch_out, duration_out = model(input)
        pitch_encoded, pitch = choose(top_k, pitch_out, pitch_vocab)
        duration_encoded, duration = choose(top_k, duration_out, duration_vocab)
        input = torch.tensor([[[pitch_encoded], [duration_encoded]]]).to(device)
        notes.append((pitch, duration))

    return notes